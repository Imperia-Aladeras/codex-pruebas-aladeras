{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3f69091",
   "metadata": {},
   "source": [
    "# PIPLEINE VERSIÓN 3 (PLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca0c31c",
   "metadata": {},
   "source": [
    "INSTALACIÓN E IMPORTACIÓN DE LIBRERÍAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0605ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy\n",
    "# pip install pandas\n",
    "# pip install scipy\n",
    "# pip install lightgbm\n",
    "# pip install dtw\n",
    "# pip install statsmodels\n",
    "# pip install sklearn\n",
    "# pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2d2bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import dtw\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "\n",
    "from scipy.stats import zscore\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.cross_decomposition import PLSRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5658600d",
   "metadata": {},
   "source": [
    "## 0. PRETRATAMIENTO GENÉRICO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f1bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_treatment(path = None, df = None, date_col_hint = None):\n",
    "    \"\"\"\n",
    "    Carga datos desde cualquier CSV/Excel o acepta DataFrame genérico.\n",
    "    Genera reporte de estructura, estadísticos, missing, outliers, distribuciones y boxplots.\n",
    "    Devuelve DataFrame procesado listo para renombrar a df_0.\n",
    "    \"\"\"\n",
    "    # Carga\n",
    "    if df is None:\n",
    "        if path.lower().endswith('.csv'):\n",
    "            df = pd.read_csv(path)\n",
    "        else:\n",
    "            df = pd.read_excel(path)\n",
    "    # Reporte inicial\n",
    "    print('Shape:', df.shape)\n",
    "    print('\\nColumnas y tipos:\\n', df.dtypes)\n",
    "    print('\\nPrimeras filas:\\n', df.head())\n",
    "    print('\\nDescriptivos numéricos:\\n', df.describe().T)\n",
    "    # Valores faltantes\n",
    "    miss = df.isnull().sum()\n",
    "    print('\\nValores faltantes por columna:\\n', miss[miss > 0])\n",
    "    # Distribuciones y boxplots\n",
    "    num_cols = df.select_dtypes(include = [np.number]).columns\n",
    "    if len(num_cols):\n",
    "        df[num_cols].hist(figsize = (15, 10), bins = 30)\n",
    "        plt.suptitle('Histogramas de variables numéricas'); plt.show()\n",
    "        for col in num_cols:\n",
    "            plt.figure()\n",
    "            df.boxplot(column = col)\n",
    "            plt.title(f'Boxplot {col}')\n",
    "            plt.show()\n",
    "    # Detección de outliers (z-score)\n",
    "    outliers = {}\n",
    "    for col in num_cols:\n",
    "        zs = zscore(df[col].dropna())\n",
    "        count = np.sum(np.abs(zs) > 3)\n",
    "        if count > 0:\n",
    "            outliers[col] = int(count)\n",
    "    print('\\nOutliers detectados (|z|>3) por columna:\\n', outliers)\n",
    "    # Inferencia de columnas de fecha y valor\n",
    "    if date_col_hint and date_col_hint in df.columns:\n",
    "        date_col = date_col_hint\n",
    "    else:\n",
    "        date_col = df.select_dtypes(include=['datetime', 'datetime64']).columns.tolist()\n",
    "        date_col = date_col[0] if date_col else df.columns[0]\n",
    "    value_cols = [c for c in df.columns if c not in [date_col] and df[c].dtype in [np.int64, np.float64]]\n",
    "    # Formato long si wide\n",
    "    if 'product_code' not in df.columns and len(value_cols) > 1:\n",
    "        df_long = df.melt(id_vars = [date_col], var_name = 'product_code', value_name = 'demand')\n",
    "    else:\n",
    "        df_long = df.rename(columns = {date_col: 'date'})\n",
    "        df_long = df_long.rename(columns = {value_cols[0]: 'demand'}) if 'product_code' in df.columns else df_long\n",
    "    df_long['date'] = pd.to_datetime(df_long['date'])\n",
    "    return df_long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542bcba9",
   "metadata": {},
   "source": [
    "## 1. CARGA Y LIMPIEZA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c56883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data(path = None, df = None, date_col = 'date', value_col = 'demand'):\n",
    "    \"\"\"\n",
    "    Carga datos desde CSV/Excel/SQL o acepta DataFrame.\n",
    "    Limpia: detecta NaNs, outliers (zscore > 3) e imputa.\n",
    "    Devuelve DataFrame long con columnas [product_code, date, demand].\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        if path.endswith('.csv'):\n",
    "            df = pd.read_csv(path)\n",
    "        else:\n",
    "            df = pd.read_excel(path)\n",
    "    df = df.copy()\n",
    "    if 'product_code' not in df.columns:\n",
    "        df = df.melt(id_vars = [date_col], var_name = 'product_code', value_name = value_col)\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    df[value_col] = df[value_col].fillna(0)\n",
    "    def cap_outliers(x):\n",
    "        zs = zscore(x)\n",
    "        x[(zs > 3) | (zs < -3)] = x.median()\n",
    "        return x\n",
    "    df[value_col] = df.groupby('product_code')[value_col].transform(cap_outliers)\n",
    "    return df[['product_code', date_col, value_col]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02536ae9",
   "metadata": {},
   "source": [
    "## 2. ANÁLISIS EXPLORATORIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc38e393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploratory_analysis(df_long, date_col = 'date', value_col = 'demand', n_top = 5):\n",
    "    \"\"\"\n",
    "    Estadísticas básicas, % ceros, ACF/PACF global y por serie.\n",
    "    Genera gráficos de distribuciones, ACF/PACF y ejemplos de series.\n",
    "    \"\"\"\n",
    "    stats = df_long.groupby('product_code')[value_col].agg(\n",
    "        mean = 'mean', var = 'var', pct_zero = lambda x: (x == 0).mean()).reset_index()\n",
    "    print(stats.head(n_top))\n",
    "\n",
    "    # Histograma de medias\n",
    "    plt.figure()\n",
    "    stats['mean'].hist(bins = 30)\n",
    "    plt.title('Distribución de medias por SKU')\n",
    "    plt.show()\n",
    "\n",
    "    # ACF/PACF global\n",
    "    y = df_long.groupby(date_col)[value_col].sum().sort_index()\n",
    "    acf_vals = acf(y, nlags = 12); pacf_vals = pacf(y, nlags = 12)\n",
    "    plt.figure()\n",
    "    plt.stem(acf_vals)\n",
    "    plt.title('ACF Global')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.stem(pacf_vals)\n",
    "    plt.title('PACF Global')\n",
    "    plt.show()\n",
    "    \n",
    "    # Muestra algunas series individuales\n",
    "    sample_skus = df_long['product_code'].unique()[:n_top]\n",
    "    plt.figure()\n",
    "    for sku in sample_skus:\n",
    "        sub = df_long[df_long['product_code'] == sku]\n",
    "        plt.plot(sub[date_col], sub[value_col], label = str(sku), alpha = 0.7)\n",
    "    plt.title('Series de muestra')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faa4957",
   "metadata": {},
   "source": [
    "## 3. CLUSTERING DE SERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa05daf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_series(df_long, k_range = range(2, 8), random_state = 42):\n",
    "    \"\"\"\n",
    "    Calcula matriz DTW entre series, selecciona k óptimo (silhouette/elbow).\n",
    "    Retorna dict sku->cluster, medoids y MDS del embedding.\n",
    "    \"\"\"\n",
    "    pivot = df_long.pivot(index = 'product_code', columns = 'date', values = 'demand').fillna(0)\n",
    "    series_list = pivot.values; N = len(series_list)\n",
    "    D = np.zeros((N, N))\n",
    "    for i in range(N):\n",
    "        for j in range(i + 1, N):\n",
    "            d = dtw.distance(series_list[i], series_list[j])\n",
    "            D[i,j] = D[j,i] = d\n",
    "    from sklearn_extra.cluster import KMedoids\n",
    "    best_k = 4  # o buscar en k_range con silhouette_score\n",
    "    km = KMedoids(n_clusters = best_k, metric = 'precomputed', random_state = random_state)\n",
    "    labels = km.fit_predict(D)\n",
    "    mapping = dict(zip(pivot.index, labels))\n",
    "    \n",
    "    # Embedding 2D\n",
    "    mds = MDS(n_components = 2, dissimilarity = 'precomputed', random_state = random_state)\n",
    "    coords = mds.fit_transform(D)\n",
    "    plt.figure()\n",
    "    plt.scatter(coords[:, 0], coords[:, 1], c = labels, cmap = 'tab10', alpha = 0.7)\n",
    "    plt.title('MDS de series por cluster')\n",
    "    plt.show()\n",
    "    \n",
    "    # Medoids\n",
    "    meds = pivot.index[km.medoid_indices_]\n",
    "    plt.figure()\n",
    "    for m in meds:\n",
    "        plt.plot(pivot.columns, pivot.loc[m], alpha = 0.8)\n",
    "    plt.title('Medoids por cluster')\n",
    "    plt.show()\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4e5d6b",
   "metadata": {},
   "source": [
    "## 4. SELECCIÓN DE LAGS POR CLUSTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167ea778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_lags_per_cluster(df_long, mapping, date_col = 'date', value_col = 'demand', acf_thresh = 0.1):\n",
    "    \"\"\"\n",
    "    Para cada cluster agrupa series, calcula ACF/PACF medias,\n",
    "    selecciona lags donde |ACF|>=acf_thresh o test de Ljung-Box.\n",
    "    Devuelve dict cluster->lista de lags.\n",
    "    \"\"\"\n",
    "    df_long_ = df_long.copy()\n",
    "    df_long_['cluster'] = df_long_['product_code'].map(mapping)\n",
    "    lags_by_cluster = {}\n",
    "    for cl in sorted(df_long_['cluster'].unique()):\n",
    "        sub = df_long_[df_long_['cluster'] == cl]\n",
    "        pivot = sub.pivot(index = date_col, columns = 'product_code', values = value_col).fillna(0)\n",
    "        acf_vals = acf(pivot.mean(axis = 1), nlags = 24)\n",
    "        pacf_vals = pacf(pivot.mean(axis = 1), nlags = 24)\n",
    "        lags = [i for i, v in enumerate(acf_vals) if abs(v) >= acf_thresh]\n",
    "        \n",
    "        # podríamos agregar condición de significancia: prueba Ljung-Box\n",
    "        lags_by_cluster[cl] = sorted(set(lags))\n",
    "        print(f\"Cluster {cl}: lags seleccionados -> {lags_by_cluster[cl]}\")\n",
    "    return lags_by_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0a0238",
   "metadata": {},
   "source": [
    "## 5. GENERACIÓN DE FEATURES DINÁMICAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185f08d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(df_long, mapping, lags_by_cluster, date_col = 'date', value_col = 'demand', h = 3):\n",
    "    \"\"\"\n",
    "    Recorre cada SKU, aplica lags dinámicos según cluster,\n",
    "    genera stats móviles, Croston y calendariales.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    for sku, cl in mapping.items():\n",
    "        series = df_long[df_long['product_code'] == sku].set_index(date_col)[value_col]\n",
    "        lags = lags_by_cluster[cl]\n",
    "        df = pd.DataFrame({'y': series})\n",
    "        for lag in lags:\n",
    "            df[f'lag_{lag}'] = df['y'].shift(lag)\n",
    "            df[f'roll_mean_{lag}'] = df['y'].shift(1).rolling(window = lag, min_periods = 1).mean()\n",
    "        \n",
    "        # Croston (como antes)\n",
    "        df = df.reset_index()\n",
    "        df['month'] = df[date_col].dt.month\n",
    "        df.dropna(inplace = True)\n",
    "        df['y_bin'] = (df['y'] > 0).astype(int)\n",
    "        df['y_reg'] = df['y']\n",
    "        features[sku] = (df.iloc[:-h], df.iloc[-h:])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7f9907",
   "metadata": {},
   "source": [
    "## 6. APLICACIÓN GLOBAL DE PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667d4321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_global_pls(features_dict, n_components = 2):\n",
    "    \"\"\"\n",
    "    Ajusta un modelo PLS sobre el conjunto de entrenamiento agregado de todas las series,\n",
    "    transforma cada train/test en componentes latentes y los añade como nuevas columnas.\n",
    "    Retorno un nuevo dict sku -> (train_df_pls, test_df_pls).\n",
    "    \"\"\"\n",
    "    # Concatenar todos los train\n",
    "    all_tr = []\n",
    "    for sku, (tr, _) in features_dict.items():\n",
    "        df = tr.copy()\n",
    "        df['sku'] = tr.copy()\n",
    "        all_tr.append(df)\n",
    "    big_tr = pd.concat(all_tr, ignore_index = True)\n",
    "    X = big_tr.drop(['date', 'y', 'y_bin', 'y_reg', 'sku'], axis = 1).values\n",
    "    y = big_tr['y_reg'].values\n",
    "    scaler = MinMaxScaler()\n",
    "    Xs = scaler.fit_transform(X)\n",
    "    pls = PLSRegression(n_components = n_components)\n",
    "    pls.fit(Xs, y)\n",
    "\n",
    "    # Transformar cada train/test\n",
    "    new_dict = {}\n",
    "    for sku, (tr, te) in features_dict.items():\n",
    "        for df_ in [tr, te]:\n",
    "            X_df = df_.drop(['date', 'y', 'y_bin', 'y_reg'], axis = 1).values\n",
    "            Xs_df = scaler.transform(X_df)\n",
    "            comps = pls.transform(Xs_df)\n",
    "            for i in range(n_components):\n",
    "                df_[f'pls_comp_{i+1}'] = comps[:, i]\n",
    "        new_dict[sku] = (tr, te)\n",
    "    return new_dict\n",
    "\n",
    "# def pls_feature_analysis(train_df, n_components=2):\n",
    "#     \"\"\"\n",
    "#     Reduce dimensiones con PLS: identifica combinaciones lineales de features\n",
    "#     que maximizan la covarianza con la variable objetivo.\n",
    "#     Grafica los primeros componentes.\n",
    "#     \"\"\"\n",
    "#     X = train_df.drop(['date','y','y_bin','y_reg'], axis=1).values\n",
    "#     y = train_df['y_reg'].values\n",
    "#     scaler = MinMaxScaler()\n",
    "#     Xs = scaler.fit_transform(X)\n",
    "#     pls = PLSRegression(n_components=n_components)\n",
    "#     comps = pls.fit_transform(Xs, y)[0]\n",
    "#     plt.figure(); plt.scatter(comps[:,0], comps[:,1], c=y, cmap='viridis', alpha=0.7)\n",
    "#     plt.title('PLS Feature Space'); plt.xlabel('Comp1'); plt.ylabel('Comp2'); plt.colorbar(label='y'); plt.show()\n",
    "#     return pls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cff126c",
   "metadata": {},
   "source": [
    "## 7. MODELOS LIGHTGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedfa7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pure_lgbm(train_df, params = None):\n",
    "    X = train_df.drop(['date', 'y', 'y_bin', 'y_reg'], axis = 1)\n",
    "    y = train_df['y_reg']\n",
    "    m = lgb.LGBMRegressor(**(params or {}))\n",
    "    m.fit(X, y)\n",
    "    return m\n",
    "\n",
    "def train_two_stage_lgbm(train_df, clf_params, reg_params):\n",
    "    X = train_df.drop(['date', 'y', 'y_bin', 'y_reg'], axis = 1)\n",
    "    clf = lgb.LGBMClassifier(**clf_params)\n",
    "    clf.fit(X, train_df['y_bin'])\n",
    "    reg = lgb.LGBMRegressor(**reg_params)\n",
    "    pos = train_df['y_bin'] == 1\n",
    "    reg.fit(X[pos], train_df.loc[pos,'y_reg'])\n",
    "    return clf, reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f6aaf3",
   "metadata": {},
   "source": [
    "## 8. BACKTEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e22b2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest(df_long, mapping, features, model_wrapper, params, h = 3):\n",
    "    recs = []\n",
    "    for sku, (tr, te) in features.items():\n",
    "        mdl = model_wrapper(tr, params)\n",
    "        X_te = te.drop(['date', 'y', 'y_bin', 'y_reg'], axis = 1)\n",
    "        if isinstance(mdl, tuple):\n",
    "            p = mdl[0].predict(X_te) * mdl[1].predict(X_te)\n",
    "        else:\n",
    "            p = mdl.predict(X_te)\n",
    "        recs.append({'sku': sku,\n",
    "                     'mae': mean_absolute_error(te['y_reg'], p),\n",
    "                     'rmse': np.sqrt(mean_squared_error(te['y_reg'], p)),\n",
    "                     'mape': np.mean(np.abs((te['y_reg'] - p) / (te['y_reg'] + 1e-6)))})\n",
    "    return pd.DataFrame(recs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920576a8",
   "metadata": {},
   "source": [
    "## 9. ANÁLISIS PLS DE FEATURES (REVISIÓN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948d84e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pls_feature_analysis(train_df, n_components = 2):\n",
    "    \"\"\"\n",
    "    Reduce dimensiones con PLS: identifica combinaciones lineales de features\n",
    "    que maximizan la covarianza con la variable objetivo. (Inspecciona covarianza con target).\n",
    "    Grafica Scree-Plot y biplot por componentes.\n",
    "    Grafica los primeros componentes.\n",
    "    \"\"\"\n",
    "    X = train_df.drop(['date', 'y', 'y_bin', 'y_reg'], axis = 1).values\n",
    "    y = train_df['y_reg'].values\n",
    "    scaler = MinMaxScaler()\n",
    "    Xs = scaler.fit_transform(X)\n",
    "    pls = PLSRegression(n_components = n_components)\n",
    "    Xs_scores, _ = pls.fit_transform(Xs, y)\n",
    "\n",
    "    # Scree-Plot de Varianza Explicada\n",
    "    var_x = np.var(Xs_scores, axis = 0)\n",
    "    var_y = np.var(pls.y_scores_, axis = 0)\n",
    "    cum_x = np.cumsum(var_x) / np.sum(var_x)\n",
    "    cum_y = np.cumsum(var_y) / np.sum(var_y)\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(cum_x) + 1), cum_x, marker = 'o', label = 'X')\n",
    "    plt.plot(range(1, len(cum_y) + 1), cum_y, marker = 'o', label = 'Y')\n",
    "    plt.title('Scree-Plot PLS')\n",
    "    plt.xlabel('Componente')\n",
    "    plt.ylabel('Varianza Explicada Acumulada')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Biplot\n",
    "    plt.figure()\n",
    "    plt.scatter(Xs_scores[:, 0], Xs_scores[:, 1], c = y, cmap = 'viridis', alpha = 0.7)\n",
    "    loadings = pls.x_loadings_\n",
    "    for i, feat in enumerate(train_df.drop(['date', 'y', 'y_bin', 'y_reg'], axis = 1).columns):\n",
    "        plt.arrow(0, 0, loadings[i, 0] * max(Xs_scores[:, 0]), loadings[i, 1] * max(Xs_scores[:, 1]), color = 'r', head_widht = 0.02)\n",
    "        plt.text(loadings[i, 0] * max(Xs_scores[:, 0]) * 1.1, loadings[i, 1] * max(Xs_scores[:, 1]) * 1.1, feat, fontsize = 8)\n",
    "    plt.title('Biplot PLS')\n",
    "    plt.xlabel('Comp1')\n",
    "    plt.ylabel('Comp2')\n",
    "    plt.colorbar(label = 'y')\n",
    "    plt.show()\n",
    "    return pls\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df2a9fd",
   "metadata": {},
   "source": [
    "## 10. EJECUCIÓN END-TO-END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3b44cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    df_long = load_and_clean_data(path = 'df_0_long.xlsx')\n",
    "    exploratory_analysis(df_long)\n",
    "    mapping = cluster_series(df_long)\n",
    "    lags_by_cluster = select_lags_per_cluster(df_long, mapping)\n",
    "    features = make_features(df_long, mapping, lags_by_cluster)\n",
    "    # 1. PLS supervisado global -> añade pls_comp_{i}\n",
    "    features_pls = apply_global_pls(features, n_components = 3)\n",
    "    # 2. Benchmark LightGBM puro\n",
    "    pure_pls = backtest(df_long, mapping, features_pls, train_pure_lgbm, {'learning_rate': 0.05, 'num_leaves': 31})\n",
    "    print('Puro + PLS: ', pure_pls.describe())\n",
    "    # 3. Benchmark Two-Stage\n",
    "    two_stage_pls = backtest(df_long, mapping, features_pls, lambda df, _: train_two_stage_lgbm(df, clf_params, reg_params), None)\n",
    "    print('Two-Stage + PLS:', two_stage_pls.describe())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
